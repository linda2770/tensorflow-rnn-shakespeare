{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Char-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Char-RNN implements multi-layer Recurrent Neural Network (RNN, LSTM, and GRU) for training/sampling from character-level language models. In other words the model takes one text file as input and trains a Recurrent Neural Network that learns to predict the next character in a sequence. The RNN can then be used to generate text character by character that will look like the original training data. This network is first posted by Andrej Karpathy, you can find out about his original code on https://github.com/karpathy/char-rnn, the original code is written in *lua*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will implement Char-RNN using Tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup\n",
    "In this part, we will read the data of our input text and process the text for later network training. There are two txt files in the data folder, for computing time consideration, we will use tinyshakespeare.txt here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "with open('tinyshakespeare.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "# length of text is the number of characters in it\n",
    "print('Length of text: {} characters'.format(len(text)))\n",
    "# and let's get a glance of what the text is\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n'   --->    0\n",
      "' '    --->    1\n",
      "'!'    --->    2\n",
      "'$'    --->    3\n",
      "'&'    --->    4\n",
      "\"'\"    --->    5\n",
      "','    --->    6\n",
      "'-'    --->    7\n",
      "'.'    --->    8\n",
      "'3'    --->    9\n",
      "':'    --->   10\n",
      "';'    --->   11\n",
      "'?'    --->   12\n",
      "'A'    --->   13\n",
      "'B'    --->   14\n",
      "'C'    --->   15\n",
      "'D'    --->   16\n",
      "'E'    --->   17\n",
      "'F'    --->   18\n",
      "'G'    --->   19\n",
      "First Citi --- characters mapped to int --- > [18 47 56 57 58  1 15 47 58 47]\n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "vocab_to_ind = {c: i for i, c in enumerate(vocab)}\n",
    "ind_to_vocab = dict(enumerate(vocab))\n",
    "text_as_int = np.array([vocab_to_ind[c] for c in text], dtype=np.int32)\n",
    "\n",
    "# We mapped the character as indexes from 0 to len(vocab)\n",
    "for char,_ in zip(vocab_to_ind, range(20)):\n",
    "    print('{:6s} ---> {:4d}'.format(repr(char), vocab_to_ind[char]))\n",
    "# Show how the first 10 characters from the text are mapped to integers\n",
    "print ('{} --- characters mapped to int --- > {}'.format(text[:10], text_as_int[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating batches\n",
    "Now that we have preprocessed our input data, we then need to partition our data, here we will use mini-batches to train our model, so how will we define our batches?\n",
    "\n",
    "Let's first clarify the concepts of batches:\n",
    "1. **batch_size**: Reviewing batches in CNN, if we have 100 samples and we set batch_size as 10, it means that we will send 10 samples to the network at one time. In RNN, batch_size have the same meaning, it defines how many samples we send to the network at one time.\n",
    "2. **sequence_length**: However, as for RNN, we store memory in our cells, we pass the information through cells, so we have this sequence_length concept, which also called 'steps', it defines how long a sequence is.\n",
    "\n",
    "From above two concepts, we here clarify the meaning of batch_size in RNN. Here, we define the number of sequences in a batch as N and the length of each sequence as M, so batch_size in RNN **still** represent the number of sequences in a batch but the data size of a batch is actually an array of size **[N, M]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TODO:</span>\n",
    "finish the get_batches() function below to generate mini-batches.\n",
    "\n",
    "Hint: this function defines a generator, use *yield*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(array, n_seqs, n_steps):\n",
    "    '''\n",
    "    Partition data array into mini-batches\n",
    "    input:\n",
    "    array: input data\n",
    "    n_seqs: number of sequences in a batch\n",
    "    n_steps: length of each sequence\n",
    "    output:\n",
    "    x: inputs\n",
    "    y: targets, which is x with one position shift\n",
    "       you can check the following figure to get the sence of what a target looks like\n",
    "    '''\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = int(len(array) / batch_size)\n",
    "    # we only keep the full batches and ignore the left.\n",
    "    array = array[:batch_size * n_batches]\n",
    "    array = array.reshape((n_seqs, -1))\n",
    "    \n",
    "    # You should now create a loop to generate batches for inputs and targets\n",
    "    #############################################\n",
    "    #           TODO: YOUR CODE HERE            #\n",
    "    #############################################\n",
    "    while True:\n",
    "        np.random.shuffle(array)\n",
    "        for n in range(0, array.shape[1], n_steps):\n",
    "            x = array[:, n:n + n_steps]\n",
    "            y = np.zeros_like(x)\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "            yield x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[50 58 57  1 51 39 63  1 57 46]\n",
      " [18 47 56 57 58  1 15 47 58 47]\n",
      " [ 1 43 52 43 51 63 11  0 37 43]\n",
      " [52 58 43 42  1 60 47 56 58 59]\n",
      " [46 47 51  1 42 53 61 52  1 58]\n",
      " [ 1 40 43 43 52  1 57 47 52 41]\n",
      " [57 47 53 52  1 53 44  1 56 43]\n",
      " [47 52  1 57 54 47 58 43  1 53]\n",
      " [56 44 53 50 49  6  0 27 52  1]\n",
      " [56 57  6  1 39 52 42  1 57 58]]\n",
      "\n",
      "y\n",
      " [[58 57  1 51 39 63  1 57 46 50]\n",
      " [47 56 57 58  1 15 47 58 47 18]\n",
      " [43 52 43 51 63 11  0 37 43  1]\n",
      " [58 43 42  1 60 47 56 58 59 52]\n",
      " [47 51  1 42 53 61 52  1 58 46]\n",
      " [40 43 43 52  1 57 47 52 41  1]\n",
      " [47 53 52  1 53 44  1 56 43 57]\n",
      " [52  1 57 54 47 58 43  1 53 47]\n",
      " [44 53 50 49  6  0 27 52  1 56]\n",
      " [57  6  1 39 52 42  1 57 58 56]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(text_as_int, 10, 10)\n",
    "x, y = next(batches)\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Build Char-RNN model\n",
    "In this section, we will build our char-rnn model, it consists of input layer, rnn_cell layer, output layer, loss and optimizer, we will build them one by one.\n",
    "\n",
    "The goal is to predict new text after given prime word, so for our training data, we have to define inputs and targets, here is a figure that explains the structure of the Char-RNN network.\n",
    "\n",
    "![structure](img/charrnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TODO:</span>\n",
    "finish all TODOs in ecbm4040.CharRNN and the blanks in the following cells.\n",
    "\n",
    "**Note: The training process on following settings of parameters takes about 20 minutes on a GTX 1070 GPU, so you are suggested to use GCP for this task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CharRNN import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Set sampling as False(default), we can start training the network, we automatically save checkpoints in the folder /checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are preset parameters, you can change them to get better result\n",
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "rnn_size = 256           # Size of hidden layers in rnn_cell\n",
    "num_layers = 2           # Number of hidden layers\n",
    "learning_rate = 0.005    # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 200  loss: 2.1357  0.2098 sec/batch\n",
      "step: 400  loss: 1.8421  0.2067 sec/batch\n",
      "step: 600  loss: 1.6938  0.2090 sec/batch\n",
      "step: 800  loss: 1.6324  0.2070 sec/batch\n",
      "step: 1000  loss: 1.6613  0.2091 sec/batch\n",
      "step: 1200  loss: 1.5557  0.2088 sec/batch\n",
      "step: 1400  loss: 1.5540  0.2076 sec/batch\n",
      "step: 1600  loss: 1.5051  0.2045 sec/batch\n",
      "step: 1800  loss: 1.4805  0.2047 sec/batch\n",
      "step: 2000  loss: 1.4816  0.2079 sec/batch\n",
      "step: 2200  loss: 1.4766  0.2048 sec/batch\n",
      "step: 2400  loss: 1.4170  0.2060 sec/batch\n",
      "step: 2600  loss: 1.4420  0.2111 sec/batch\n",
      "step: 2800  loss: 1.4236  0.2084 sec/batch\n",
      "step: 3000  loss: 1.4270  0.2036 sec/batch\n",
      "step: 3200  loss: 1.4300  0.2069 sec/batch\n",
      "step: 3400  loss: 1.4077  0.2095 sec/batch\n",
      "step: 3600  loss: 1.4409  0.2076 sec/batch\n",
      "step: 3800  loss: 1.3666  0.2087 sec/batch\n",
      "step: 4000  loss: 1.4060  0.1993 sec/batch\n",
      "step: 4200  loss: 1.3896  0.2030 sec/batch\n",
      "step: 4400  loss: 1.3794  0.2090 sec/batch\n",
      "step: 4600  loss: 1.3901  0.2036 sec/batch\n",
      "step: 4800  loss: 1.3278  0.2104 sec/batch\n",
      "step: 5000  loss: 1.3979  0.2071 sec/batch\n",
      "step: 5200  loss: 1.3574  0.2010 sec/batch\n",
      "step: 5400  loss: 1.3770  0.2060 sec/batch\n",
      "step: 5600  loss: 1.3349  0.2050 sec/batch\n",
      "step: 5800  loss: 1.3728  0.2038 sec/batch\n",
      "step: 6000  loss: 1.3743  0.2061 sec/batch\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(vocab), batch_size, num_steps, 'LSTM', rnn_size,\n",
    "               num_layers, learning_rate)\n",
    "batches = get_batches(text_as_int, batch_size, num_steps)\n",
    "model.train(batches, 6000, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i6000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i4000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i6000_l256.ckpt\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up checkpoints\n",
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "Set the sampling as True and we can generate new characters one by one. We can use our saved checkpoints to see how the network learned gradually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i6000_l256.ckpt\n",
      "['L' 'O' 'R' ... 't' 'h' 'e']\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(vocab), batch_size, num_steps,'LSTM', rnn_size,\n",
    "               num_layers, learning_rate, sampling=True)\n",
    "# choose the last checkpoint and generate new text\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = model.sample(checkpoint, 1000, len(vocab), vocab_to_ind, ind_to_vocab, prime='LORD ')\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LORD these sealst\n",
      "And towned at your power, and as you dear,\n",
      "To be by honour of thyself were so.\n",
      "\n",
      "KATHARINA:\n",
      "I'll save, sir! to hear her and borts\n",
      "Than's mouged throops: who thoughts he will not stand,\n",
      "This is the place and though the seat of sigh storm\n",
      "Talked for sordlus things, thou and the sunder\n",
      "Of honour there to bear me at him to-norder.\n",
      "\n",
      "GLOUCESTER:\n",
      "Wyaved soul shall spate the stear'n trurceit of thee.\n",
      "The lorder to tantuadigrument,\n",
      "Be sorry to my sperits, to be so.\n",
      "I say then the words, something strange and tender.\n",
      "\n",
      "GLOUCESTER:\n",
      "What so then I am trances on them as her friar?\n",
      "This sirk am that within, I whould river on,\n",
      "As thou shouldst strew subject that a them on to him.\n",
      "\n",
      "CLIFFORD:\n",
      "Ay, this's too, brief him what thou hast boughtly,r' words.\n",
      "\n",
      "CAMILLO:\n",
      "To be to mean to say her chambicger men.\n",
      "\n",
      "Provost:\n",
      "What, to the tears that I have houes in that\n",
      "I have to seal a corit of his bowt seems sovereign,\n",
      "I must be so too, till to bade these love.\n",
      "\n",
      "GLOUCESTER:\n",
      "Thy marky stredce there and the"
     ]
    }
   ],
   "source": [
    "for i in samp:\n",
    "    print(i,end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i2000_l256.ckpt\n",
      "['L' 'O' 'R' ... ' ' 't' 'h']\n"
     ]
    }
   ],
   "source": [
    "# choose a checkpoint other than the final one and see the results. It could be nasty, don't worry!\n",
    "#############################################\n",
    "#           TODO: YOUR CODE HERE            #\n",
    "#############################################\n",
    "model = CharRNN(len(vocab), batch_size, num_steps,'LSTM', rnn_size,\n",
    "               num_layers, learning_rate, sampling=True)\n",
    "checkpoint = \"checkpoints/i2000_l256.ckpt\"\n",
    "samp = model.sample(checkpoint, 1000, len(vocab), vocab_to_ind, ind_to_vocab, prime=\"LORD \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LORD and then,\n",
      "The holdird of your silk off and hat to me.\n",
      "\n",
      "KONH HENRY 'T \n",
      "We will she's save a person to much\n",
      "Worliep of a stand.\n",
      "\n",
      "COMINIUS:\n",
      "Ay, beater you a wife to save them blded.\n",
      "\n",
      "MISRRASLANC:\n",
      "If you do no more an enome, and thy command shoods.\n",
      "\n",
      "GLOUCESTER:\n",
      "As it send them so barren of my hang,\n",
      "The sin of this some son we have the part of\n",
      "hen all so dare'd; therefore, that I, the hardes should have.--\n",
      "The day it should be so houseness. I'll stain and heart out\n",
      "For her the fount a capt of hall is now the\n",
      "concersed warmarit to him, in the morning\n",
      "Wing thee not shall be he a shepter, and burthen\n",
      "thisers is thrings to thought thy here, sir, bry thee no\n",
      "confestion: bear a mother's prayers of that?\n",
      "\n",
      "CORINA:\n",
      "He would not speak it, well to have some holy menes.\n",
      "\n",
      "PETRUCHIO:\n",
      "One as I am with more other sad too, he'se the\n",
      "wood traison\n",
      "And seem on those and strange, and stand to lia\n",
      "To should so more thee here. \n",
      "MENENIUS:\n",
      "Thy sine as art to the more merd, bedore.\n",
      "\n",
      "POMPEY:\n",
      "Went to my love, indeed th"
     ]
    }
   ],
   "source": [
    "for i in samp:\n",
    "    print(i,end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change another type of RNN cell\n",
    "We are using LSTM cell as the original work, but GRU cell is getting more popular today, let's chage the cell in rnn_cell layer to GRU cell and see how it performs. Your number of step should be the same as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: You need to change your saved checkpoints' name or they will rewrite the LSTM results that you have already saved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 200  loss: 2.3189  0.1972 sec/batch\n",
      "step: 400  loss: 1.9622  0.1946 sec/batch\n",
      "step: 600  loss: 1.7934  0.1942 sec/batch\n",
      "step: 800  loss: 1.7167  0.1926 sec/batch\n",
      "step: 1000  loss: 1.7364  0.1967 sec/batch\n",
      "step: 1200  loss: 1.6092  0.1934 sec/batch\n",
      "step: 1400  loss: 1.5947  0.1899 sec/batch\n",
      "step: 1600  loss: 1.5388  0.1984 sec/batch\n",
      "step: 1800  loss: 1.5021  0.1947 sec/batch\n",
      "step: 2000  loss: 1.5135  0.1939 sec/batch\n",
      "step: 2200  loss: 1.4950  0.2016 sec/batch\n",
      "step: 2400  loss: 1.4478  0.1950 sec/batch\n",
      "step: 2600  loss: 1.4506  0.1888 sec/batch\n",
      "step: 2800  loss: 1.4231  0.1963 sec/batch\n",
      "step: 3000  loss: 1.4404  0.1977 sec/batch\n",
      "step: 3200  loss: 1.4371  0.1953 sec/batch\n",
      "step: 3400  loss: 1.4168  0.2040 sec/batch\n",
      "step: 3600  loss: 1.4430  0.1972 sec/batch\n",
      "step: 3800  loss: 1.3672  0.1949 sec/batch\n",
      "step: 4000  loss: 1.4248  0.1948 sec/batch\n",
      "step: 4200  loss: 1.3980  0.1957 sec/batch\n",
      "step: 4400  loss: 1.3859  0.1975 sec/batch\n",
      "step: 4600  loss: 1.3882  0.1932 sec/batch\n",
      "step: 4800  loss: 1.3350  0.1964 sec/batch\n",
      "step: 5000  loss: 1.3922  0.1919 sec/batch\n",
      "step: 5200  loss: 1.3460  0.1995 sec/batch\n",
      "step: 5400  loss: 1.3646  0.1927 sec/batch\n",
      "step: 5600  loss: 1.3241  0.1942 sec/batch\n",
      "step: 5800  loss: 1.3541  0.1935 sec/batch\n",
      "step: 6000  loss: 1.3798  0.1928 sec/batch\n"
     ]
    }
   ],
   "source": [
    "# these are preset parameters, you can change them to get better result\n",
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "rnn_size = 256           # Size of hidden layers in rnn_cell\n",
    "num_layers = 2           # Number of hidden layers\n",
    "learning_rate = 0.005    # Learning rate\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size, num_steps, 'GRU', rnn_size,\n",
    "               num_layers, learning_rate)\n",
    "batches = get_batches(text_as_int, batch_size, num_steps)\n",
    "model.train(batches, 6000, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i6000_l256.ckpt\n",
      "['L' 'O' 'R' ... 'h' 'o' 'u']\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(vocab), batch_size, num_steps, 'GRU', rnn_size,\n",
    "               num_layers, learning_rate, sampling=True)\n",
    "# choose the last checkpoint and generate new text\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = model.sample(checkpoint, 1000, len(vocab), vocab_to_ind, ind_to_vocab, prime=\"LORD \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LORD and tears\n",
      "Or that thou layst and thought, and tell into my lage,\n",
      "It may be bring to some mind and to my lark unto\n",
      "Our truth of his contemth buckle farment too;\n",
      "Teans and the strengmed and mercy in hist title and\n",
      "What should I be my life of him served thing to me arrink,\n",
      "I have men in the mighty sovereign low.\n",
      "\n",
      "HORTENSIO:\n",
      "The care of the statorury of the bought\n",
      "Of daughal far a brother, with the fall such any here!\n",
      "\n",
      "DUKE OF AUMENOESBELLE: I have I make her:\n",
      "To her and fortune's hand is so bear too say.\n",
      "I say ' of trinustuly my honour is!\n",
      "O than men! I would shame me fair, but served\n",
      "In thy mutis a terdal. Thy miful through torture\n",
      "Being sending be corn friends. But who darly side?\n",
      "\n",
      "BAPOIABLALdENCE:\n",
      "Went a pardon a cale, sir, and is not the leisure.\n",
      "\n",
      "HORTENSIO:\n",
      "To how there will straint, my too mass light, they,\n",
      "And to my house, my liege, thy loves, taught me,\n",
      "Is son a wook, to make o desert from him,\n",
      "To make me speak wilm out of my life;\n",
      "And art I will not, for my father should;\n",
      "And thou"
     ]
    }
   ],
   "source": [
    "for i in samp:\n",
    "    print(i,end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. Compare your result of two networks that you built and the reasons that caused the difference. (It is a qualitative comparison, it should be based on the specific model that you build.)\n",
    "2. Discuss the difference between LSTM cells and GRU cells, what are the pros and cons of using GRU cells?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "**\n",
    "1.Comparison:\n",
    "GRU is faster than LSTM. It took LSTM more than 0.2 sec to train a batch, while for GRU it is less than 0.2 sec. But for the result, LSTM is better than GRU. Those words LSTM made make more sense than GRU's.\n",
    "\n",
    "2.The difference:\n",
    "GRU has 2 gates: update gate z and reset gate r; LSTM has 3: input i, forget f, and output o.\n",
    "In the LSTM unit, the amount of the memory content that is seen, or used by other units in the network is controlled by the\n",
    "output gate. On the other hand the GRU exposes its full content without any control.\n",
    "Another difference is in the location of the input gate, or the corresponding reset gate. The LSTM\n",
    "unit computes the new memory content without any separate control of the amount of information\n",
    "flowing from the previous time step. Rather, the LSTM unit controls the amount of the new memory\n",
    "content being added to the memory cell independently from the forget gate. On the other hand, the\n",
    "GRU controls the information flow from the previous activation when computing the new, candidate\n",
    "activation, but does not independently control the amount of the candidate activation being added.\n",
    "\n",
    "pros of GRU: GRU have fewer parameters and thus may train a bit faster or need less data to generalize.\n",
    "cons of GRU: with large data, the LSTMs with higher expressiveness may lead to better results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
